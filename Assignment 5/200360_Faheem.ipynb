{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Section 2\n",
    "\n",
    "## SGLD\n",
    "\n",
    "This section mainly deals with Stochastic Gradient Langevian Diffusion. Langevian diffusion is based on providing fluctuations into a system. How this is used in a system becomes apparent once we consider the differential equation, which contaains a drift term and a brownian term.\n",
    "\n",
    "We start with the introduction of the potential term. For our usage, the posterior density of a distribution can be represented in terms of potential as follows.\n",
    "$$\n",
    "\\pi(\\theta) \\propto exp(-U(\\theta))\n",
    "$$\n",
    "Considering $\\pi(\\theta)$ as the posterior, and $y_1, ... y_N$ as samples, we can represent $U(\\theta)$ as \n",
    "$$\n",
    "U(\\theta) = \\Sigma _{i = 1}^N U_i(\\theta)\n",
    "$$\n",
    "where \n",
    "$$\n",
    "U_i(\\theta) = -log(f(y_i|\\theta)) - \\frac{1}{N} log(p(\\theta))\n",
    "$$\n",
    "\n",
    "Where $f$ is the likelihood function and $p(\\theta)$ is the prior. Now, langevin diffusion states that $ d\\theta(t) = -1/2 \\nabla U(\\theta(t))dt + dB_t $.\n",
    "Here, the first term is the drift, while the second term is the brownian factor, providing randomness.\n",
    "If we are using the langevian diffusion algorithm for getting new samples of the possible values of parameters, then we can perform the following.\n",
    "$ \\theta _k = \\theta _{k-1} + h/2 \\nabla U(\\theta _k) + \\xi $.\n",
    "\n",
    "However, the computation of $\\nabla U(\\theta)$ will be computatuonally intense as we have to sum across all the elements in the given dataset, which becomes especially challenging if the dataset is extremely large. So to overcome this challenge, SGLD uses only a subset of non repeating n elements from the dataset, each time we calculate the gradient. Here, n << N. Therefore, the algorithm for SGLD is as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: $\\theta_0, \\{h_0, ... , h_K\\}$.\n",
    "\n",
    "for $k \\in 1, ... , K$ do:\n",
    "\n",
    "   1. Draw $S_n \\subset {1, ... , N}$ without replacement\n",
    "\n",
    "   2. Estimate $\\hat{\\nabla} U(\\theta)^{(n)}$ \n",
    "\n",
    "   3. Draw $\\xi _k$ ∼ $N(0, h_kI)$\n",
    "\n",
    "   4. Update $\\theta_k+1 ← \\theta _k − \\frac{h_k}{2} \\hat{\\nabla} U(\\theta _k)^{(n)} + \\xi _k$\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $\\hat{\\nabla} U(\\theta) = \\frac{N}{n} \\Sigma _{i \\in S_n} \\nabla U_i(\\theta) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SGLD, computing the gradient is not efficient. Therefore, we use estimates to replace the true gradient. While using estimates, its important to choose estimates which reduce the variance. We use control variates for exactly this purpose.\n",
    "$$\n",
    "\\Sigma _{i=1}{N} \\nabla U_i(\\theta) = \\Sigma _{i=1}{N} \\nabla u_i(\\theta) + \\frac{N}{n} \\Sigma _{i\\in S_n} (\\nabla U_i(\\theta) - u_i(\\theta))\n",
    "$$\n",
    "Control variates improves time complexity from $O(N)$ to $O(1) $, assuming we already know $\\hat{\\theta} $.\n",
    "However, this approach increases variance, as $\\hat{\\theta} $ can vary significantly from $\\theta$. We therefore have 2 approaches to reduce variance. One is by only accepting $\\hat{\\theta} $ only if its sufficiently close to $\\theta$. Another method is using prefferential sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3\n",
    "\n",
    "In the previous section, we had seen how SGLD is used as a stochastic gradient MCMC algorithm. In this section, we provide a general framework for SGMCMC algorithms, which includes SGLD, along with others.\n",
    "\n",
    "We first declare $\\zeta$, which contains $\\theta$. However, it can also contain a velocity component, $\\rho$. Therefore, the general stochastic equation for $\\zeta$ is\n",
    "$$\n",
    "d\\zeta = \\frac{1}{2}b(\\zeta)dt + \\sqrt{D(\\zeta)}dB_t\n",
    "$$\n",
    "\n",
    "Here, $b(\\zeta)$ becomes the drift component, and D takes a similar role to the gaussian noice provided in SGLD. In order to represent b, we introduce two new terms, the function $H(\\zeta) $ and $Q(\\zeta) $, latter of which is skew symmetric. \n",
    "We write $b(\\zeta) $ as\n",
    "$$\n",
    "b(\\zeta) = -[D(\\zeta) + Q(\\zeta)]\\nabla H(\\zeta) + \\Gamma(\\zeta)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\Gamma _i(\\zeta) = \\Sigma _{j=1}^d \\frac{\\delta}{\\delta \\zeta}(D_{ij}(\\zeta) + Q_{ij}(\\zeta))\n",
    "$$\n",
    "\n",
    "Therefore, our new samples for the mean from the distribution can be taken by using \n",
    "$$\n",
    "\\zeta_{t+h} \\approx \\zeta_{t} - \\frac{h}{2}[[D(\\zeta) + Q(\\zeta)]\\nabla H(\\zeta) + \\Gamma(\\zeta)] + \\sqrt{h}Z\n",
    "$$\n",
    "where h represents the sampling time and $Z$ is sampled from $N(0, \\zeta_t) $. In order to avoid inflation of variance, we use $V(\\theta_t) $. We also need to change $Z$ in order to counter this inflation of variance. However, by changing $Z$, we have to make sure variance of $N(0, D(\\zeta_t) - \\hat{B}(\\zeta_t)) $, isn't less that zero, else the result will become unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### MNIST dataset training using Bayesian Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg;\n",
    "Pkg.add(\"CSV\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"Distributions\")\n",
    "# Pkg.add(\"TensorFlow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "using Distributions\n",
    "# using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>label</th><th>1x1</th><th>1x2</th><th>1x3</th><th>1x4</th><th>1x5</th><th>1x6</th><th>1x7</th><th>1x8</th><th>1x9</th><th>1x10</th><th>1x11</th></tr><tr><th></th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>60,000 rows × 785 columns (omitted printing of 773 columns)</p><tr><th>1</th><td>5</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>2</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>3</th><td>4</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>4</th><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>5</th><td>9</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>6</th><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>7</th><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>8</th><td>3</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>9</th><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>10</th><td>4</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>11</th><td>3</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>12</th><td>5</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>13</th><td>3</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>14</th><td>6</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>15</th><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>16</th><td>7</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>17</th><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>18</th><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>19</th><td>6</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>20</th><td>9</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>21</th><td>4</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>22</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>23</th><td>9</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>24</th><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccccccc}\n",
       "\t& label & 1x1 & 1x2 & 1x3 & 1x4 & 1x5 & 1x6 & 1x7 & 1x8 & 1x9 & 1x10 & 1x11 & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & Int64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 5 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t3 & 4 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t4 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t5 & 9 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t6 & 2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t7 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t8 & 3 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t9 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t10 & 4 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t11 & 3 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t12 & 5 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t13 & 3 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t14 & 6 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t15 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t16 & 7 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t17 & 2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t18 & 8 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t19 & 6 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t20 & 9 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t21 & 4 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t22 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t23 & 9 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t24 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "60000×785 DataFrame. Omitted printing of 777 columns\n",
       "│ Row   │ label │ 1x1   │ 1x2   │ 1x3   │ 1x4   │ 1x5   │ 1x6   │ 1x7   │\n",
       "│       │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m │\n",
       "├───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n",
       "│ 1     │ 5     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │\n",
       "│ 2     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │\n",
       "│ 3     │ 4     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │\n",
       "│ 4     │ 1     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │\n",
       "│ 5     │ 9     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │\n",
       "│ 6     │ 2     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │\n",
       "│ 7     │ 1     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │\n",
       "⋮\n",
       "│ 59993 │ 9     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │\n",
       "│ 59994 │ 5     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │\n",
       "│ 59995 │ 1     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │\n",
       "│ 59996 │ 8     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │\n",
       "│ 59997 │ 3     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │\n",
       "│ 59998 │ 5     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │\n",
       "│ 59999 │ 6     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │\n",
       "│ 60000 │ 8     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = CSV.read(\"mnist_train.csv\", DataFrame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have written the softmax function referring to the internet. We use the function to get the probabilities of each integer using the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softMax (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_exp(x::AbstractVecOrMat) = exp.(x .- maximum(x))\n",
    "\n",
    "_sftmax(e::AbstractVecOrMat) = (e ./ sum(e))\n",
    "\n",
    "function softMax(X::AbstractVecOrMat{T})::AbstractVecOrMat where T<:AbstractFloat\n",
    "    _sftmax(_exp(X))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logLikelihood (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function logLikelihood(X, y, A, B, a, b)\n",
    "    beta = softMax(X*B .+ b')\n",
    "    beta = softMax(beta*A' .+ a')\n",
    "    return beta\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also declared random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000×10 Matrix{Float64}:\n",
       " 1.3346e-6  1.65383e-6  9.98647e-7  …  3.16154e-6  1.33368e-6  4.85932e-7\n",
       " 1.3346e-6  1.65383e-6  9.98647e-7     3.16154e-6  1.33368e-6  4.85932e-7\n",
       " 1.3346e-6  1.65383e-6  9.98647e-7     3.16154e-6  1.33368e-6  4.85932e-7\n",
       " 1.3346e-6  1.65383e-6  9.98647e-7     3.16154e-6  1.33368e-6  4.85932e-7\n",
       " 1.3346e-6  1.65383e-6  9.98647e-7     3.16154e-6  1.33368e-6  4.85932e-7\n",
       " 1.3346e-6  1.65383e-6  9.98647e-7  …  3.16154e-6  1.33368e-6  4.85932e-7\n",
       " 1.3346e-6  1.65383e-6  9.98647e-7     3.16154e-6  1.33368e-6  4.85932e-7\n",
       " 1.3346e-6  1.65383e-6  9.98647e-7     3.16154e-6  1.33368e-6  4.85932e-7\n",
       " 1.3346e-6  1.65383e-6  9.98647e-7     3.16154e-6  1.33368e-6  4.85932e-7\n",
       " 1.3346e-6  1.65383e-6  9.98647e-7     3.16154e-6  1.33368e-6  4.85932e-7\n",
       " ⋮                                  ⋱                          \n",
       " 1.3346e-6  1.65383e-6  9.98647e-7     3.16154e-6  1.33368e-6  4.85932e-7\n",
       " 1.3346e-6  1.65383e-6  9.98647e-7     3.16154e-6  1.33368e-6  4.85932e-7\n",
       " 1.3346e-6  1.65383e-6  9.98647e-7     3.16154e-6  1.33368e-6  4.85932e-7\n",
       " 1.3346e-6  1.65383e-6  9.98647e-7     3.16154e-6  1.33368e-6  4.85932e-7\n",
       " 1.3346e-6  1.65383e-6  9.98647e-7  …  3.16154e-6  1.33368e-6  4.85932e-7\n",
       " 1.3346e-6  1.65383e-6  9.98647e-7     3.16154e-6  1.33368e-6  4.85932e-7\n",
       " 1.3346e-6  1.65383e-6  9.98647e-7     3.16154e-6  1.33368e-6  4.85932e-7\n",
       " 1.3346e-6  1.65383e-6  9.98647e-7     3.16154e-6  1.33368e-6  4.85932e-7\n",
       " 1.3346e-6  1.65383e-6  9.98647e-7     3.16154e-6  1.33368e-6  4.85932e-7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = data[1:60000, 2:785]\n",
    "X = Matrix(X)\n",
    "y = data[1:60000, 1:1]\n",
    "A = rand(Normal(), 10, 100)\n",
    "B = rand(Normal(), 784, 100)\n",
    "B = Matrix(B)\n",
    "a = rand(Normal(), 10)\n",
    "b = rand(Normal(), 100)\n",
    "lambdaA = rand(Gamma(1,1))\n",
    "lambdaB = rand(Gamma(1,1))\n",
    "lambdaa = rand(Gamma(1,1))\n",
    "lambdab = rand(Gamma(1,1))\n",
    "beta = logLikelihood(X, y, A, B, a, b)\n",
    "# beta[1][225]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to train the model using that data. I had an issue with the training part as there were doubts regarding the SGLD method which are yet to be resolved.\n",
    "\n",
    "Essentially, we will use the SGLD method whereby we select a subset of the dataset for updating one parameter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
